

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Summary}
\begin{itemize}
\item Hypothesis testing for regression coefficients is analogous to hypothesis testing for the population mean: Use the $t$-statistic to calculate the $p$-values and either accept or reject the null hypothesis. 
\item Like a confidence interval for the population mean, a $95\%$ confidence interval for a regression coefficient is computed as the estimator $\pm 1.96 \times$ standard errors.
\item When $X$ is binary, the regression model can be used to estimate and test hypotheses about the difference between the population means of the ``$X=0$'' group and the ``$X=1$'' group.
\item In general, the error $u_i$ is heteroskedastic; that is, the variance of $u_i$ at a given value of $X$, $\var(u_i|X_i=x)$, depends on x. 
\item The error is homoskedastic if $\var{u_i|X_i=x}$ is constant. Heteroskedasticity-robust standard errors produce valid statistical inference.
\item If the three least squares assumption hold \textit{and} if the regression errors are homoskedastic, then the Gauss-Markov theorem implies that the OLS estimator is BLUE.
\item If the three least squares assumptions hold, if the regression errors are homoskedastic, and if the regression errors are normally distributed, then the OLS $t$-statistic computed using homoskedasticity-only standard errors has a Student-$t$ distribution under the null. 
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


